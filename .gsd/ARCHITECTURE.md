# Architecture

> Auto-generated by /map on 2026-01-19

## Overview

This is a hybrid desktop application for sports betting analysis. It combines a **React/Vite** frontend with a **Rust/Tauri** backend for local processing and data storage, augmented by **Supabase Edge Functions** for AI-powered analysis.

The system scrapes sports data (Premier League stats from FBref), stores it in a local SQLite database, and provides tools for analyzing matches and betting picks.

```mermaid
graph TD
    User[User] --> GUI[React Frontend]
    GUI -- "Tauri Invoke" --> App[Tauri Backend (Rust)]
    GUI -- "REST" --> LegacyAPI[Supabase Edge Functions]
    
    subgraph "Local Desktop Environment"
        App --> SQLite[(sports_data.db)]
        App --> Scraper[Scraper Module]
        Scraper -- "Http" --> ExtWeb[External Web (FBref)]
    end
    
    subgraph "Cloud / External"
        LegacyAPI --> Gemini[Gemini AI API]
    end
```

## Components

### Frontend (`src/`)
- **Purpose:** User interface for dashboard, match listings, and analysis details.
- **Framework:** React + Vite + TailwindCSS + Shadcn UI.
- **Key Pages:**
    - `DashboardHome.tsx`: Main view, calls `get_all_matches`.
    - `MatchDetail.tsx`: Detailed analysis.
- **Integration:** communicates with Rust via `@tauri-apps/api`.

### Backend: Tauri App (`src-tauri/app-desktop`)
- **Purpose:** Main desktop application entry point.
- **Key Commands:**
    - `get_all_matches`: Retrieves match history from SQLite.
    - `fetch_upcoming_matches`: Triggers scraping of fixtures.
    - `analyze_missing_teams`: Scrapes history for specific teams to fill gaps.

### Backend: Scraper Bot (`src-tauri/scraper-bot`)
- **Purpose:** Standalone binary for background data collection.
- **Function:** Crawls league data and populates the shared SQLite database.

### Backend: Shared Lib (`src-tauri/shared-lib`)
- **Purpose:** Core logic shared between App and Bot.
- **Modules:**
    - `db`: SQLite schema and operations.
    - `analysis`: Match prediction logic.
    - `fixtures` & `match_stats`: Scraping logic.

### Edge Functions (`supabase/functions`)
- **Component:** `analyze-pick`
- **Purpose:** AI analysis of specific user bets using Google Gemini.
- **Trigger:** Direct HTTP call from frontend (via Supabase client).

## Data Flow

1.  **Data Ingestion:**
    *   User clicks "Sync" or Bot runs -> Scrapes FBref -> SQLite `matches` / `teams` tables.
2.  **View Layer:**
    *   Frontend `invoke('get_all_matches')` -> Query SQLite -> Return JSON to React.
3.  **Analysis:**
    *   **Local:** `invoke('get_match_analysis')` -> Rust calculates stats based on DB data.
    *   **AI:** `supabase.functions.invoke('analyze-pick')` -> Sends props to Deno function -> Calls Gemini -> Returns text analysis.

## Integration Points

| Service | Type | Purpose |
|---------|------|---------|
| **FBref.com** | Web Parsing | Source of truth for match stats and fixtures. |
| **Google Gemini** | API | Generative AI analysis for betting picks. |
| **Supabase** | Backend-as-a-Service | Edge Functions hosting, potentially Auth (configured but extent of use unclear). |
| **SQLite** | Local DB | Primary storage for scraped sports data. |

## Technical Debt

- [ ] **Hardcoded Credentials:** Gemini API key fallback exists in source code (`analyze-pick`).
- [ ] **Lazy Migrations:** Database schema changes (`ALTER TABLE`) are executed in Rust code instead of a migration system.
- [ ] **Inconsistent Scraping:** Scraping logic exists in both `scraper-bot` and `app-desktop` (via `shared-lib`), potentially leading to race conditions if both run.
- [ ] **Placeholder Data:** Frontend uses hardcoded probabilities (`85%`) until analysis is complete.
- [ ] **Error Handling:** Backend frequently prints errors to stdout (`println!`) rather than structured logging or UI feedback.
